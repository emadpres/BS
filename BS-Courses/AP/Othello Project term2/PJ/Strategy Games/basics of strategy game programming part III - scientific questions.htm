<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<!-- saved from url=(0033)http://www.fierz.ch/strategy5.htm -->
<HTML><HEAD><TITLE>basics of strategy game programming: part III - scientific questions</TITLE>
<META content="text/html; charset=windows-1252" http-equiv=Content-Type>
<META name=GENERATOR content="MSHTML 8.00.7600.16385">
<META name=FORMATTER content="Arachnophilia 3.2"></HEAD>
<BODY leftMargin=0 topMargin=0 bgColor=#ffffff text=#000000>
<TABLE border=0 cellSpacing=0 cellPadding=20 width="100%" bgColor=#000000>
  <TBODY>
  <TR>
    <TD><FONT color=#bbbbbb size=6 face=Ventura><B>Strategy Game 
      Programming</B> </FONT></TD>
    <TD align=right><FONT color=#ff0000 size=6 face=Ventura><B>Outlook</B> 
      </FONT></TD></TR></TBODY></TABLE>
<TABLE border=0 cellSpacing=0 cellPadding=20 width="100%" bgColor=#ffffff>
  <TBODY>
  <TR>
    <TD>
      <H4>Scientific and Philosophical Questions, Links</H4>This last part of my 
      strategy game programming tutorial focuses on some questions which turned 
      up during my programming sessions. Many of them have no clear answers, but 
      are just some observations I made along the way. I find them interesting - 
      I hope you do too. 
      <UL>
        <LI><A href="http://www.fierz.ch/strategy5.htm#speed">The need for 
        speed</A> 
        <LI><A href="http://www.fierz.ch/strategy5.htm#dumborsmart">Smart and 
        slow or dumb and fast?</A> 
        <LI><A href="http://www.fierz.ch/strategy5.htm#learning">Machine 
        learning</A> 
        <LI><A href="http://www.fierz.ch/strategy5.htm#improving">Improving your 
        program</A> 
        <LI><A href="http://www.fierz.ch/strategy5.htm#links">Other sites on 
        strategy game programming</A> </LI></UL><A name=speed>
      <H4>The need for speed</H4>Game programmers are obsessive about the speed 
      of their programs. Many would sell the soul of their first-born child for 
      another 10% speed increase. Why is this? The reason for this craze over a 
      few % is that it does make a difference. In fact, it makes a larger 
      difference than you might think. In a famous experiment, Ken Thompson 
      found that his chess program "Belle" gained 200 elo points (a measure of 
      playing strength) for every additional ply it searched. It means get an 
      average of 75% of the points when you play an opponent with 200 points 
      less. Everybody tried this experiment, and everybody found the same 
      result. So people started speculating how many ply you need to be world 
      champion. They also started speculating about "diminishing returns": the 
      notion that once you have a certain search depth, you will not gain much 
      more by searching another ply. This diminishing returns thing has been 
      looked for in chess, but it proved to be harder to find than expected. I 
      looked for this effect with an old version of my checkers program, Cake++, 
      probably in about the year 2000 or 2001: I used fixed-depth versions of 
      Cake++ which searched to a specific number of plies ahead, 5,7,9, and so 
      on, and always let two versions with a 2 ply difference play each other. I 
      just repeated this experiment (August 2006) with my latest version of Cake 
      (Cake Manchester 1.09d) with the 8-piece endgame database. In my old 
      experiment, Cake++ was only using the 6-piece database. I have plotted the 
      winning percentage of the deeper-searching engine vs its search depth for 
      Chinook, Cake++ and Cake Manchester in the following graph. <BR>The 
      Chinook data is probably taken from <A 
      href="http://www.cs.ualberta.ca/~jonathan/Papers/Papers/BFSearch.ps">A 
      Re-examination of Brute-force Search</A>, Jonathan Schaeffer, Paul Lu, 
      Duane Szafron and Robert Lake, Games: Planning and Learning, AAAI 1993 
      Fall Symposium, Report FS9302, pages 51-58. <BR>Unfortunately, I can't 
      open this PostScript document any more and therefore I can't check whether 
      I took the data from that paper. I wonder if it's just my PC or if it is 
      corrupted? 
      <CENTER><!--
<table cellspacing="15" border="1">
<tr>
<td>Search Depth</td><td> Result (Cake++) </td><td>win-% </td><td> sigma </td><td> Result
(Chinook)</td>
</tr><tr>
<td>5-3 </td><td> +196 =53 -33 </td><td> 78.9% </td><td> 2.1% </td><td> -- </td></tr><tr>
<td> 7-5 </td><td> +153 =100 -29 </td><td> 72.0% </td><td> 2.0% </td><td> 77.5%<td></tr><tr>
<td> 9-7 </td><td> +181 =75 -26 </td><td> 77.5% </td><td> 2.0% </td><td> 65.0%<td></tr><tr>
<td> 11-9 </td><td> +130 =111 -41 </td><td> 65.8% </td><td> 2.1% </td><td> 72.5%<td></tr><tr>
<td> 13-11 </td><td> +134 =116 -32 </td><td> 68.1% </td><td> 2.0% </td><td> 58.8%<td></tr><tr>
<td>15-13 </td><td> +119 =136 -27 </td><td> 66.3% </td><td> 1.9% </td><td> 58.8%<td></tr><tr>
<td> 17-15 </td><td> +89 = 165 -28 </td><td> 60.8% </td><td> 1.8% </td><td> 61.3%<td></tr><tr>
<td> 19-17 </td><td> +78 = 176 -28 </td><td> 58.9% </td><td> 1.8% </td><td> 57.5%<td></tr><tr>
<td> 21-19 </td><td> +60 = 189 -33 </td><td> 54.8% </td><td> 1.7% </td><td> 57.5% <td></tr></table>
--><IMG alt="graph of diminishing returns" 
      src="basics%20of%20strategy%20game%20programming%20part%20III%20-%20scientific%20questions_files/diminishingreturns2006.gif"> 
      </CENTER>As you can see, the effect of diminishing returns is obviously 
      visible from the graph, for the Chinook experiment and both experiments 
      with Cake. However, the Chinook experiment is has an unclear end - for 
      high search depths, the winning percentage of the stronger engine remains 
      nearly constant, and the returns don't seem to diminish at all. Perhaps 
      this is due to the too small statistical sample. In my matches, I played 
      288 games per match. For both experiments with Cake, the result is very 
      clear, and the returns diminish further the deeper the engines search. 
      <P>Another thing which I find interesting is that the winning percentage 
      of the deeper-searching Cake Manchester is consistently lower than for 
      Cake++ (with the exception of the first two data points for 5-3 and 7-5 
      ply). I believe this effect can be explained easily: Cake Manchester is 
      much stronger than Cake++ was. One might argue that Cake Manchester 
      searching 13 ply ahead is about as good as Cake++ searching 17 ply ahead. 
      13 and 17 are just arbitrary guesses, but in any case it is true that Cake 
      Manchester searching D ply ahead is about as strong as Cake++ searching 
      D+X ply ahead, with X&gt;0. In this sense, the entire curve in the graph 
      is shifted; to use the numbers before, the 13-11 data point for Cake 
      Manchester would be similar to the 17-15 data point of Cake++. Another way 
      to look at this is to not only look at the winning percentage, but at the 
      match result in terms of wins, draws and losses. At the same search depth, 
      there are many more draws in the Cake Manchester match, because this 
      engine is playing much better.
      <P>
      <CENTER>
      <TABLE border=1 cellSpacing=0 cellPadding=4>
        <TBODY>
        <TR>
          <TD>Search Depth </TD>
          <TD>Cake++ </TD>
          <TD>CakeManchester </TD></TR>
        <TR>
          <TD>5-3 </TD>
          <TD>+196=53-33 </TD>
          <TD>+220=52-16 </TD></TR>
        <TR>
          <TD>7-5 </TD>
          <TD>+153=100-29 </TD>
          <TD>+174=94-20 </TD></TR>
        <TR>
          <TD>9-7 </TD>
          <TD>+181=75-26 </TD>
          <TD>+150=112-25 </TD></TR>
        <TR>
          <TD>11-9 </TD>
          <TD>+130=111-41 </TD>
          <TD>+105=165-18 </TD></TR>
        <TR>
          <TD>13-11 </TD>
          <TD>+134=116-32 </TD>
          <TD>+85=180-23 </TD></TR>
        <TR>
          <TD>15-13 </TD>
          <TD>+119=136-27 </TD>
          <TD>+81=189-18 </TD></TR>
        <TR>
          <TD>17-15 </TD>
          <TD>+89=165-28 </TD>
          <TD>+46=231-11 </TD></TR>
        <TR>
          <TD>19-17 </TD>
          <TD>+78=176-28 </TD>
          <TD>+36=238-14 </TD></TR>
        <TR>
          <TD>21-19 </TD>
          <TD>+60=189-33 </TD>
          <TD>+25=255-8 </TD></TR></TBODY></TABLE></CENTER><A name=dumborsmart>
      <H4>Dumb and fast or smart and slow?</H4>A related question to the 
      diminishing returns thing is this: can a program compensate for missing 
      knowledge by speed? Again, there is a classic experiment by Hans Berliner 
      on this topic, who used his chess program Hitech to answer this question. 
      He crippled Hitech by disabling large parts of the evaluation function, 
      and produced a program called Lotech in this way. Now, he played Lotech 
      against Hitech, giving Lotech the advantage of searching one ply deeper. 
      The surprising result is that Lotech won consistently against Hitech. 
      Again, people think that there should be a "crossover point", where the 
      Hitech version of a program should beat the Lotech version. And again, in 
      computer chess, it proved hard to find. I also looked for this with my 
      checkers program, but at the time (again with Cake++ about 2001), I also 
      found that Lotech-Cake would consistently beat Hitech-Cake. <A 
      name=learning>
      <H4>Machine learning</H4>Machine learning is a fascinating topic. There 
      are many different ways in which computer programs try to make use from 
      past experience, and many of them are called "learning" by the program 
      authors. Let's take a look: The most popular "learning" is rote learning: 
      Your program plays a game, loses the game, and remembers it lost this 
      game. This can be implemented as book learning, where your program will 
      flag a book move as bad, if it loses a game with it and chooses another 
      move next time round. Or it can be implemented in the form of a small 
      hashtable, where positions from games it lost are stored as lost. Like 
      this, the program will not play into this position again. But: this form 
      of learning is <EM>very</EM> specific. It is too specific! Your checkers 
      program might know that one specific position is bad. But what if a whole 
      class of positions is to blame which your program does not understand? In 
      checkers, there are the famous "holds". In such a situation, e.g. 3 
      checkers of yours are held by 2 of your opponent. Regardless of what the 
      rest of the situation is on the board, (more or less) he will win, because 
      it's like being a checker up. Your basic rote learning program will see 
      that one position with this hold is lost - but there are millions of 
      others like it! And it will play the same game again, varying slightly 
      only to find itself lost again. This type of learning is no real learning 
      and I consider it pure hype to call such programs "intelligent" or 
      "learning from mistakes". <BR>Another type of machine learning uses 
      automated evaluation weight adjustment. This is the domain of genetic 
      algorithms: genetic algorithms make use of evolution theory found in 
      nature to find the "fittest" program. Here's how it works: You produce a 
      (typically linear) evaluation function, where every term has a weight, 
      that is:<BR>eval = w_1*f_1 + w_2*f_2 + ... + w_n*f_n<BR>the f_i are 
      features of the position. Example: in checkers, you could define f_1 as 
      black has a strong back rank. Now, the larger w_1 is, the more your 
      program will try to keep it's back rank intact. So once you have selected 
      your set of features, you go ahead and try to optimize the weights. The 
      genetic approach does this by selecting a number of programs with initial 
      weights at random, and then lets them play each other to find which ones 
      are good and which are bad. A number of the bad programs gets zapped, and 
      the fit programs survive and breed offspring - either by generating a 
      cross of two succesful programs (taking each w_i at random from two fit 
      programs), or by random mutation of a single fit program. Now you just 
      continue this process, and hope that you get a really good set of weights 
      in the end. This is approximately the approach Arthur Samuel took in his 
      famous checkers program in the 1950's. There are other types of weight 
      fitting: the Deep Blue team used a huge set of positions from grandmaster 
      games, and let the program crunch on them. They measured the number of 
      times the program chose the same move as the grandmaster. Then they varied 
      the weights, and tried again. If it solved more positions, they kept the 
      changes, else they discarded them. The last weight-fitting approach I know 
      of was used by Michael Buro in his world champion Othello program, 
      Logistello. He used a huge database of positions, which were scored 
      according to the result of the game. Then, he used his evaluation function 
      on all these positions to optimize it to reproduce the scores as well as 
      possible. <BR>All of these approaches are already much a better form of 
      learning than rote learning. Here, if a program has a feature for such a 
      hold in checkers, you will find that increasing the weight of this term in 
      the evaluation will really make the program know that a whole class of 
      positions is bad, and it will avoid them. So here, do we have learning? I 
      say no! What if this feature had never been in the program? It could have 
      adjusted weights as long as it wanted to, and it would never have gotten 
      anywhere. Therefore, we get to the next step in learning:<BR>Neural 
      Networks can "invent" patterns that nobody has told them of. NNs can be 
      used to make an evaluation of board positions and be incorportated into a 
      normal game playing program. NNs consist of a number of inputs, some 
      optional hidden layers, and an output. The layers connecting the inputs 
      and the output are again a number of nodes which have input connections 
      from the layer before, and output connections to the next layer. A node 
      can have any number of connections to the other layers. Each connection 
      can be stronger or weaker. By simultaneously changing the connections and 
      their strenghts, you can do both pattern recognition and have a weight to 
      each pattern. <BR>NNs can be trained by giving them scored test positions, 
      or they can even learn everything by themselves, by combining the NN with 
      a genetic approach to find the fittest NN. Blondie24, a checkers program, 
      uses this very approach, and it learned to play checkers relatively well, 
      all by itself! This, to me, is true learning. It is similar to the way 
      humans think. The authors of Blondie claim a bit more about it than what 
      is true, but unfortunately that is true for most researchers today - you 
      need to produce lots of publicity to get funding... <A name=improving>
      <H4>Improving your program</H4>You spent countless hours writing your 
      program, and now it plays like a beginner? That is more common than the 
      opposite, but don't despair: I have a number of tips for you! First and 
      foremost: the most problems arise because of bugs in your code. The 
      AlphaBeta algorithm has the insidious property of hiding bugs: for a 
      million positions evaluated in your program, all you get is one move. Even 
      if you return random numbers for 10% of your evaluations, your program 
      will still play decently most of the time. Therefore, you need to take 
      some extra measures to be sure everything is working ok. One typical 
      example is a test of your evaluation symmetry: If your eval doesn't return 
      the same number for a position and the color-reversed twin, then something 
      is wrong. If your board has other symmetries (in chess you have a mirror 
      operation through the middle half of the board between d- and e-file), you 
      can use these too. Try writing as clear code as possible - don't optimize 
      too early, it isn't worth while. Don't add all bells and whistles to your 
      program before it plays its first regular game on a plain alphabeta 
      algorithm. Once you have eradicated your bugs, you will want to start 
      tuning it. Is it too slow? You can use profilers such as AMD's CodeAnalyst 
      to find the hotspots in your program and work on them. A very useful tool 
      to improve a program is to have it play against other programs. Play 
      matches with hundreds of games, anything else is not statistically 
      significant. Change something in your code (in your search, in your eval), 
      and replay the matches to confirm your changes are good. If you only 
      changed something in the move ordering, use a test suite instead of a long 
      match, it takes much less time and tells you just as well whether your new 
      move ordering is an improvement or not. <A name=links>
      <H4>Other sites on strategy game programming</H4>
      <UL>
        <LI><A 
        href="http://www-db.stanford.edu/pub/voy/museum/samuel.html">Arthur 
        Samuel</A> 
        <LI><A href="http://www-anw.cs.umass.edu/~rich/book/the-book.html">A 
        book on reinforcement learning</A> 
        <LI><A 
        href="http://www.shef.ac.uk/psychology/gurney/notes/contents.html">Neural 
        Networks</A> 
        <LI><A 
        href="http://supertech.lcs.mit.edu/~heinz/dt/node49.html">Self-play 
        experiments (Hitech-Lotech, Belle, others)</A> 
        <LI><A href="http://www.seanet.com/~brucemo/topics/topics.htm">Bruce 
        Moreland's pages on chess programming</A> 
        <LI><A href="http://members.home.nl/matador/chess840.htm">Ed Schröder's 
        pages on chess programming</A> 
        <LI><A href="http://www.digenetics.com/">Digenetics website 
        (Blondie24)</A> </LI></UL><BR>Comments and questions are welcome!
      <HR color=#888888>
       
      <CENTER>[ <A href="http://www.fierz.ch/">Author homepage</A> | <A 
      href="http://www.fierz.ch/strategy.htm">Introduction</A> | <A 
      href="http://www.fierz.ch/strategy1.htm">Part I</A> | <A 
      href="http://www.fierz.ch/strategy2.htm">Part II</A> | <A 
      href="http://www.fierz.ch/strategy3.htm">Part III</A> | <A 
      href="http://www.fierz.ch/strategy4.htm">Part IV</A> | <A 
      href="http://www.fierz.ch/strategy5.htm">Part V</A> ] 
      <P>Last update: August 13, 2006, using <A 
      href="http://www.arachnoid.com/"><IMG border=0 alt=arachnoid align=middle 
      src="basics%20of%20strategy%20game%20programming%20part%20III%20-%20scientific%20questions_files/arachno.gif"></A> 
      </CENTER></P></TD></TR></TBODY></TABLE></BODY></HTML>
